---
title: "CITS4009 - Project2"
author: "Jiaheng GU (23925667) (50%): all tasks and Shijun SHAO (23926903) (50%): all tasks"
output: html_document
---

Video Link: https://youtu.be/lf-0zw-vROE

# Introduction

The project is to build classification and clustering models based on the Youtube dataset. The data set can be obtained from <https://www.kaggle.com/datasets/nelgiriyewithana/global-youtube-statistics-2023?resource=download>

This dataset gives information about subscriber counts, video views, upload frequency, country of origin, earnings, and more.

Load libraries

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(fpc)
library(forcats)
library(ggplot2)
library(gridExtra)
library(grDevices)
library(lime)
library(pROC)
library(purrr)
library('ROCR')
library(ROCit)
library('rpart')
library(scales)
library(stringr)
```

Load dataset and observe the variables

```{r}
data.path <- 'D:/WA/Study/2023 S2/CITS4009/Project/youtube_UTF_8.csv'
youtube <- read.csv(data.path)
str(youtube)
```

We can see that the data is not consistent and there are some missing values.

In addidition, there are some variables that not relevant to our analysis, so in the following steps we need to deal with the irrelevant variable.

# Part 1: Data cleaning and transformation

```{r}
# Replace all values with 'nan' or 'NAN' with NA
youtube[youtube == "nan" | youtube == "NAN"] <- NA

# Delete duplicate rows
youtube <- youtube[!duplicated(youtube), ]

# Replace 0 with NA
youtube <- mutate_all(youtube, ~replace(., . == 0, NA))

# Filter out rows containing more than 60% NA
youtube <- youtube %>%
  filter(rowSums(is.na(.)) / ncol(.) <= 0.6)

# If it is a numerical variable, replace the NA value with mean, and if it is a text variable, replace NA with 'Missing'
num_cols <- c("subscribers", "video.views", "uploads", "created_year", 
              "Gross.tertiary.education.enrollment....", "Population", 
              "Unemployment.rate", "Urban_population", "Latitude", "Longitude", "lowest_monthly_earnings", "highest_monthly_earnings", "lowest_yearly_earnings", "highest_yearly_earnings")

chr_cols <- c("category", "Country", "channel_type", "created_month")

for (col in num_cols) {
  youtube[[col]][is.na(youtube[[col]])] <- mean(youtube[[col]], na.rm = TRUE)
}

for (col in chr_cols) {
  youtube[[col]][is.na(youtube[[col]])] <- "Missing"
}
for (col in chr_cols) {
  youtube[[col]] <- as.factor(youtube[[col]])
}

# Delete created_ Rows with 'Missing' in the month column
youtube <- youtube[!(youtube$created_month == "Missing"), ]

# Filter out unreasonable years. YouTube was created in 2005 and delete data from before 2005
youtube <- youtube %>%
  filter(`created_year` >= 2005)

# Delete inappropriate lines, such as the highest yearly earning being less than the lowest early earning
youtube <- youtube %>%
  filter(
    !(
      highest_yearly_earnings < lowest_monthly_earnings | 
      highest_yearly_earnings < highest_monthly_earnings |
      highest_yearly_earnings < lowest_yearly_earnings
    )
  )

# View youtube data after cleaning
summary(youtube)
```

Convert the created_month column to a numeric type, e.g. "Jan" to 1.

```{r}
month_mapping <- data.frame(
  Month = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
  created_Month = 1:12
)

youtube <- youtube %>%
  left_join(month_mapping, by = c(created_month = "Month")) %>%
  mutate(created_month = coalesce(created_Month, median(created_Month, na.rm = TRUE))) %>%
  select(-created_Month) 
```

# Part 2: Classification

### Selecting the Response (Target) Variable

We want to classify YouTube channels as either "high-earning" or "low-earning" as response(target) Variable.
Upon examining our data, we noticed that monthly earnings, both at their peaks and lows, can fluctuate widely. These might not depict a channel's yearly performance accurately. While the "lowest yearly earnings" gives us a baseline, the "highest yearly earnings" helps capture the potential of a channel. Therefore, we chose to work with the "highest yearly earnings."
To make our analysis more nuanced, we decided to relate these earnings to the GDP of the country where the channel is located. This is crucial because earning $1,000 in a high-income country might be ordinary, but in a low-income country, it might be substantial. By comparing with the GDP, we can gauge the true significance of a channel's earnings.

Breaking down our approach:
We made sure that country names in our datasets were consistent.
We created a function to fetch the GDP for each country, which offers insight into the country's economic health.
We enhanced our YouTube dataset with a GDP column, putting the channel's earnings in perspective with their country's economic performance.
We introduced an "earnings_multiplier" column, which is the difference between the log of the channel's highest yearly earnings and the log GDP of their country. This helps us see how a channel fares relative to their nation's economy.
Using this multiplier, we categorized channels. If their multiplier was above a certain threshold (like the upper quartile), we labeled them as "high-earning."
In essence, our approach was to ensure the classifications truly represent the channel's earnings in the context of where they operate.

```{r}
# Load GDP per capita data
gdp_data <- read.csv("API_NY.GDP.csv", skip=4, stringsAsFactors = FALSE, fileEncoding="utf-8")

# Define a mapping function to standardize country names
map_country_name <- function(name) {
  if (name == "south korea") return("korea rep")
  if (name == "russia") return("russian federation")
  if (name == "turkey") return("turkiye")
  if (name == "venezuela") return("venezuela rb")
  if (name == "egypt") return("egypt arab rep")
  return(name)
}

# Define a function to clean country names
clean_country_name <- function(name) {
  name <- tolower(name) # Convert to lowercase
  name <- gsub("[^a-z0-9 ]", "", name) # Remove special characters
  name <- gsub(" +", " ", trimws(name)) # Trim white space and replace multiple spaces with a single space
  name <- map_country_name(name) # Map to standard name
  return(name)
}

# Apply the cleaning functions
gdp_data$Country.Name <- sapply(gdp_data$Country.Name, clean_country_name)
youtube$Country <- sapply(youtube$Country, clean_country_name)


# Define a function to obtain log GDP per capita for a country
get_log_gdp_per_capita <- function(country_name) {
  if(country_name == "missing") {
    avg_gdp <- mean(gdp_data$X2022, na.rm = TRUE)
    return(log(avg_gdp))
  }
  
  country_gdp_data <- gdp_data[gdp_data$Country.Name == country_name, ]
  
  # Check if matching country data is found
  if(nrow(country_gdp_data) == 0) {
    cat("Country not found:", country_name, "\n")
    return(NA)
  }
  
  # Grab GDP in year order and take logarithms
  for(year in 2022:1960) {
    gdp_value <- country_gdp_data[[paste0("X", year)]]
    if (!is.na(gdp_value) && gdp_value > 0) {
      log_gdp_per_capita <- log(gdp_value)
      return(log_gdp_per_capita)
    }
  }
  return(NA)
}

# Calculate the earnings_multiplier for each channel
youtube <- youtube %>%
  mutate(
    log_country_gdp = map_dbl(Country, get_log_gdp_per_capita),
    log_highest_yearly_earnings = log(highest_yearly_earnings),
    earnings_multiplier = ifelse(is.na(log_country_gdp), NA, log_highest_yearly_earnings - log_country_gdp)
  )

# Determine the threshold for high-earning channels
quantile_75 <- quantile(youtube$earnings_multiplier, 0.75, na.rm = TRUE)

# Label channels as high-earning (1) or low-earning (0)
youtube <- youtube %>%
  mutate(
    earning_category = ifelse(earnings_multiplier > quantile_75, 1, 0)
  )


# Check the data structure and summary
str(youtube$earning_category)
table(youtube$earning_category)
```
This table shows the distribution of high and low earnings.

### Selecting Variables

In our quest to predict the income categorization of YouTube channels, we've identified specific features from the dataset based on their potential relevance and influence on the outcome:

Subscribers: Subscriber count often correlates with earnings; more subscribers typically denote a larger viewership, potentially leading to increased ad revenue. 

Video views: Directly related to earnings, as views correlate with ad impressions and revenue generation. Category: Channel genre or theme might influence ad rates and viewer engagement. 

Uploads: A reflection of channel activity and, potentially, its popularity. Country: Different regions may have varying ad rates and viewer purchasing power. 

Channel type: Channel categories might influence ad rates and the type of engagement from viewers. Created year: Offers insights into the age of the channel. Older channels might have established a more solid audience base, possibly correlating with higher earnings. 

Created month: Reveals potential seasonal patterns in channel creation. Certain months may favor the inception and growth of new channels, influencing their earnings trajectory. Gross tertiary education enrollment (%): Can relate to viewer engagement and ad-click rates. 

Population: Larger population could signify a more significant viewer base. Unemployment rate: Might be linked to the amount of online time and viewer engagement. Urban population: Relates to the target market for many advertisers. 

Latitude and Longitude: The inclusion of latitude and longitude serves to capture regional nuances in earnings. Even within the same country, there can be significant income variations based on geographical factors. These coordinates, while broad, allow our model to recognize and potentially leverage these geographical variations.

Columns Not Selected and Justifications: 
Rank, Title and Youtuber:These are unique identifiers and, therefore, unlikely to provide generalizable patterns for our predictive model. 

Abbreviation: This is essentially a redundant feature if we already include "Country." 

Video views for the last 30 days and Subscribers for the last 30 days: While these columns reflect recent trends, they might introduce noise due to their short-term nature. They can capture transient spikes in views or subscribers, which might not persist in the long term. While recent trends can be insightful, for a robust model that predicts long-term income classification, these might be less stable indicators. 

Created date: While providing exact creation dates, it may not add significant predictive power after accounting for month and year, and could introduce unnecessary complexity. 

other earnings: Based on the calculation, we find that the correlations are high so they are removed.

```{r}
# Calculate lowest_ Monthly_ Earnings and Highest_ Yearly_ The correlation of earnings
cor1 <- cor(youtube$lowest_monthly_earnings, youtube$highest_yearly_earnings, use = "complete.obs")
# Calculate Highest_ Monthly_ Earnings and Highest_ Yearly_ The correlation of earnings
cor2 <- cor(youtube$highest_monthly_earnings, youtube$highest_yearly_earnings, use = "complete.obs")
# Calculate lowest_ Yearly_ Earnings and Highest_ Yearly_ The correlation of earnings
cor3 <- cor(youtube$lowest_yearly_earnings, youtube$highest_yearly_earnings, use = "complete.obs")

cat("Correlation between lowest_monthly_earnings and highest_yearly_earnings: ", cor1, "\n")
cat("Correlation between highest_monthly_earnings and highest_yearly_earnings:", cor2, "\n")
cat("Correlation between lowest_yearly_earnings and highest_yearly_earnings:  ", cor3, "\n")
```

```{r}
# Select specific columns as candidate and response variables
youtube <- youtube %>%
  select(subscribers, video.views, category, uploads, Country, channel_type, created_year, created_month,
         `Gross.tertiary.education.enrollment....`, Population, `Unemployment.rate`, 
         Urban_population, Latitude, Longitude, earning_category)
outcome <- "earning_category"
pos.label <- "1"
```

## The Null Model

Firstly, we check the null model which provides a simple baseline to describe the average value or distribution of the response variable in the absence of any predictor variables or features.

```{r}
Npos <- sum(youtube[,"earning_category"] == 1)
pred_Null <- Npos / nrow(youtube)

cat("Proportion of high-earning in the data:", pred_Null)

```

### Splitting the data

Splitting the data into training and test sets is crucial for assessing model performance, preventing overfitting, tuning hyperparameters, selecting the best model, and ensuring that our model can generalize to new data effectively.

We split the data into a training set and a test set(90,10).

```{r}
set.seed(4009)  # Ensure repeatability
vars = setdiff(colnames(youtube), c("earning_category","group"))

# Split data into category variables and numerical variables
catVars = vars[sapply(youtube[, vars], class) %in%
c('character','factor')]

numericVars = vars[sapply(youtube[, vars], class) %in%
c('numeric','integer')]

youtube$group = runif(dim(youtube)[1])
trainingSet = subset(youtube, group<= 0.9)
test_set = subset(youtube, group>0.9)
```

Now, we further split the training set into a train set and a calibration set(80,20).

```{r}
calib.set = rbinom(dim(trainingSet)[1], size=1, prob=0.2)>0
calibration_set = subset(trainingSet, calib.set)
train_set = subset(trainingSet, !calib.set)
```

## Single Variable Models

A single-variable model based on categorical features is easiest to describe as a table. First of all, we observe one of the single variable as an example.

```{r}
outcome <- "earning_category"
pos = '1' # We are interested in when 'earning_category' is positive. We need to put quotes around number 1 as it is the column name of the table created

table.1 = table(trainingSet[,'channel_type'], trainingSet[,outcome], useNA='ifany')
print(table.1[,2]/(table.1[,1]+table.1[,2]))
```

We can see that the channel type Animals has 0.33 probability of outputting a 1, comedy has 0.28 probability.

### Single Variable Models with Categorical Variables

Category variables

```{r}
catVars
```

In this section, we utilized a custom function mkPredC to compute prediction probabilities based on individual categorical variables. This function calculates the proportion of positive cases (e.g., high-earning channels) within each categorical level, and then applies these proportions to the corresponding categorical levels in other datasets.

```{r}
mkPredC <- function(outCol,varCol,appCol, pos=pos.label) {
  pPos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol),varCol)
  pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

Following this, we executed this prediction function for each categorical variable, producing a new column of prediction probabilities.

These new prediction probability columns reflect the potential predictive power of each categorical variable.

```{r}
# Call the mkPredC() function for all the categorical columns
for (v in catVars) {
  pred_var <- paste('pred', v, sep='')
  train_set[, pred_var] <- mkPredC(train_set[, outcome],train_set[, v], train_set[, v])
  calibration_set[, pred_var] <- mkPredC(train_set[, outcome], train_set[, v], calibration_set[, v])
  test_set[, pred_var] <- mkPredC(train_set[, outcome], train_set[, v], test_set[, v])
}
```

Checking some of the results.

```{r}
rows <- c(123,108,98,65,27,19,9)
calibration_set[rows,c("category","predcategory")]
```

From the above results,we can see that the Entertainment observations return the same value.

Then, we employed another function calcAUC to compute the AUC values for these single-variable predictions. The AUC provides us with a consistent measure of the predictive performance of our models, where 1 represents perfect prediction and 0.5 denotes random guessing.

```{r}
calcAUC <- function(predcol, outcol, pos=pos.label) {
  perf <- performance(prediction(predcol, outcol==pos),'auc')
  as.numeric(perf@y.values)
}
```

```{r}
betcatVars = c()
for (v in catVars) {
  pred_var <- paste('pred', v, sep='')
  aucTrain <- calcAUC(train_set[, pred_var], train_set[, outcome])
  # The calibration set AUC is calculated and the results are printed only when the training set AUC is greater than or equal to 0.54
  if (aucTrain >= 0.54) {
    aucCal <- calcAUC(calibration_set[, pred_var], calibration_set[, outcome])
    print(sprintf(
      "%s: trainAUC: %4.3f; calibrationAUC: %4.3f",
      pred_var, aucTrain, aucCal
    ))
    betcatVars = c(betcatVars, v)
  }
}
```

### 10-Fold Cross-Validation

Let's inspect the AUC values of above categorical columns a bit more.

```{r}
for (var in betcatVars) {
aucs <- rep(0,10)
for (rep in 1:length(aucs)) {
useForCalRep <- rbinom(n=nrow(trainingSet), size=1, prob=0.1) > 0
predRep <- mkPredC(trainingSet[!useForCalRep, outcome],
trainingSet[!useForCalRep, var],
trainingSet[useForCalRep, var])
aucs[rep] <- calcAUC(predRep, trainingSet[useForCalRep, outcome])
}
print(sprintf("%s: mean: %4.3f; sd: %4.3f", var, mean(aucs), sd(aucs)))
}
```

Given the 10 Fold CV, we can see that most of the variables were good, there is no significant drop in the AUC, even the variable Country improved.

Also, the standard deviation of the CVs are quite small, so all the variables contribute to the model.

### Double Density Plots

```{r}
str(factor(train_set[,'channel_type']))
str(factor(train_set[,'category']))
str(factor(train_set[,'Country']))
fig1 <- ggplot(calibration_set) + geom_density(aes(x=predchannel_type, color=as.factor(earning_category)))
fig2 <- ggplot(calibration_set) + geom_density(aes(x=predcategory, color=as.factor(earning_category)))
fig3 <- ggplot(calibration_set) + geom_density(aes(x=predCountry, color=as.factor(earning_category)))
grid.arrange(fig1,fig2,fig3,ncol=1)

```

1.Analysis of the predchannel_type Density Plot: This graph displays the distribution of predicted probabilities based on the channel type. Observationally, there is an overlap in the distribution of prediction probabilities for both low-earning (earning_category = 0) and high-earning (earning_category = 1) channels. This suggests that classifying income solely based on the channel_type poses certain challenges. 

2.Analysis of the predcategory Density Plot: Similar to the previous chart, here we observe the predicted probability distribution based on the category of the channel. The chart suggests that there is a distinct separation in the predicted probabilities for high-earning and low-earning channels, though some overlap still exists. This implies that the category of the channel might offer some contribution to the predictive power in classifying high versus low earnings.

3.Analysis of the predCountry Density Plot: For the country of the channel's origin, the predicted probability distributions for high-earning and low-earning channels are very close to each other. This might indicate that the country characteristic might not be a strong predictor in differentiating between high and low earning channels.

### Single Variable Models with Numerical Variables

Numerical Variables

```{r}
numericVars
```

Discretisation: Binning numeric into categorical

```{r}
q1 <- quantile(train_set[,"subscribers"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varsub = cut(train_set[,"subscribers"], unique(q1))
q2 <- quantile(train_set[,"video.views"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varvideo = cut(train_set[,"video.views"], unique(q2))
q3 <- quantile(train_set[,"uploads"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varuploads = cut(train_set[,"uploads"], unique(q3))
q4 <- quantile(train_set[,"created_year"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varyear = cut(train_set[,"created_year"], unique(q4))
q5 <- quantile(train_set[,"created_month"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varmonth = cut(train_set[,"created_month"], unique(q5))
q6 <- quantile(train_set[,"Gross.tertiary.education.enrollment...."], probs=seq(0, 1, 0.1), na.rm=T)  
dis.Varedu = cut(train_set[,"Gross.tertiary.education.enrollment...."], unique(q6))
q7 <- quantile(train_set[,"Population"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varpop = cut(train_set[,"Population"], unique(q7))
q8 <- quantile(train_set[,"Unemployment.rate"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varrate = cut(train_set[,"Unemployment.rate"], unique(q8))
q9 <- quantile(train_set[,"Urban_population"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varurban = cut(train_set[,"Urban_population"], unique(q9))
q10 <- quantile(train_set[,"Latitude"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varla = cut(train_set[,"Latitude"], unique(q10))
q11 <- quantile(train_set[,"Longitude"], probs=seq(0, 1, 0.1), na.rm=T)
dis.Varlo = cut(train_set[,"Longitude"], unique(q11))
```

```{r}
mkPredN <- function(outCol,varCol,appCol) {
  cuts <- unique(as.numeric(quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

```{r}
betnumericVars = c()
for(v in numericVars) {
  pred_var<-paste('pred',v,sep='')
  train_set[,pred_var] <- mkPredN(train_set[,outcome], train_set[,v], train_set[,v])
  test_set[,pred_var] <- mkPredN(train_set[,outcome], train_set[,v], test_set[,v])
  calibration_set[,pred_var] <- mkPredN(train_set[,outcome], train_set[,v], calibration_set[,v])
  aucTrain <- calcAUC(train_set[,pred_var],train_set[,outcome])
  
  if(aucTrain>=0.54) {
    aucCal<-calcAUC(calibration_set[,pred_var],calibration_set[,outcome])
    print(sprintf(
      "%s, trainAUC: %4.3f calibrationAUC: %4.3f",
      pred_var,aucTrain,aucCal))
    betnumericVars = c(betnumericVars, v)
  }
}
```

### 10-fold Cross-Validation

```{r}
for (var in betnumericVars) {
aucs <- rep(0,10)
for (rep in 1:length(aucs)) {
useForCalRep <- rbinom(n=nrow(trainingSet), size=1, prob=0.1) > 0
predRep <- mkPredN(trainingSet[!useForCalRep, outcome],
trainingSet[!useForCalRep, var],
trainingSet[useForCalRep, var])
aucs[rep] <- calcAUC(predRep, trainingSet[useForCalRep, outcome])
}
print(sprintf("%s: mean: %4.3f; sd: %4.3f", var, mean(aucs), sd(aucs)))
}

```

Given the 10 Fold CV, we can see that most of the variables were good, there is no significant drop in the AUC, even the variables such as longitude improved.

Also, the standard deviation of the CVs are quite small, so all the variables contribute to the model.

### Double Density Plot

```{r}
fig4 <- ggplot(calibration_set) + geom_density(aes(x=predsubscribers, color=as.factor(earning_category)))
fig5 <- ggplot(calibration_set) + geom_density(aes(x=predvideo.views, color=as.factor(earning_category)))
fig6 <- ggplot(calibration_set) + geom_density(aes(x=preduploads, color=as.factor(earning_category)))
fig7 <- ggplot(calibration_set) + geom_density(aes(x=predcreated_year, color=as.factor(earning_category)))
fig8 <- ggplot(calibration_set) + geom_density(aes(x=predcreated_month, color=as.factor(earning_category)))
fig9 <- ggplot(calibration_set) + geom_density(aes(x=predGross.tertiary.education.enrollment...., color=as.factor(earning_category)))
grid.arrange(fig4, fig5, fig6, fig7, fig8, fig9, ncol=1)
```

1.Analysis of the predsubscribers Density Plot: This graph presents the density distribution of predicted subscribers for both high-earning and low-earning channels. The plot indicates that channels with a higher number of subscribers are more likely to fall within the high-earning category. However, there's a region of overlap where channels, irrespective of their subscriber count, can belong to either earning category. This overlap implies that while subscribers can be an indicative factor, they are not the sole determinant for a channel's earnings.

2.Analysis of the predvideo.views Density Plot: The plot visualizes the density distribution of predicted video views for the two earning categories. It's evident that channels with higher video views have a greater propensity to be in the high-earning bracket. Nevertheless, a region of overlap suggests that not all channels with high views necessarily earn more, and other factors could come into play.

3.Analysis of the preduploads Density Plot: This visualization depicts the density distribution for predicted uploads between the earning categories. The data suggests that channels that upload content more frequently lean more towards the high-earning category. Yet, the overlap region again indicates that upload frequency alone might not be a definitive marker for earnings.

4.Analysis of the predcreated_year Density Plot: This chart presents the distribution of channels based on the year they were created. The denser regions of the distribution for the high-earning category in more recent years might imply that newer channels have a higher chance of earning more. Conversely, older channels tend to populate the low-earning bracket more densely.

5.Analysis of the predcreated_month Density Plot: Here, the predicted distribution focuses on the month of channel creation. The relatively uniform distribution across the months for both earning categories suggests that the month of creation doesn't have a significant impact on a channel's earnings potential.

6.Analysis of the predGross.tertiary.education.enrollment Density Plot: This plot emphasizes the distribution based on tertiary education enrollment rates. It appears that channels originating from regions with higher tertiary education enrollment rates might be more inclined towards the high-earning category. This could point towards cultural or regional factors influencing a channel's earning potential.

```{r}
fig10 <- ggplot(calibration_set) + geom_density(aes(x=predPopulation, color=as.factor(earning_category)))
fig11 <- ggplot(calibration_set) + geom_density(aes(x=predUnemployment.rate, color=as.factor(earning_category)))
fig12 <- ggplot(calibration_set) + geom_density(aes(x=predUrban_population, color=as.factor(earning_category)))
fig13 <- ggplot(calibration_set) + geom_density(aes(x=predLatitude, color=as.factor(earning_category)))
fig14 <- ggplot(calibration_set) + geom_density(aes(x=predLongitude, color=as.factor(earning_category)))
grid.arrange(fig10, fig11, fig12, fig13, fig14, ncol=1)
```

1: Analysis of the predPopulation Density Plot: The distribution suggests that there's minimal distinction between high-earning (1) and low-earning (0) channels concerning the population factor. Both categories appear to span a similar range of predicted populations.

2: Analysis of the predUnemployment.rate Density Plot: It is observed that channels from areas with a mid-range unemployment rate (~0.5) have a slight lean towards the high-earning category. However, the overlap in the extremes indicates that unemployment rate alone might not be a predominant factor affecting earnings.

3: Analysis of the predUrban_population Density Plot: It appears that channels from moderately urbanized areas have a nearly equal distribution between high and low earnings. There's substantial overlap, suggesting that the urban population percentage isn't a major determinant for a channel's earnings in this dataset.

4: Analysis of the predLatitude Density Plot: It's evident that channels from areas closer to the equator (latitude ~0.2) tend to be more in the low-earning category. However, the distribution is relatively consistent, indicating that while latitude might play a role, it isn't the primary driver for earnings.

5: Analysis of the predLongitude Density Plot: There's a noticeable peak in the high-earning category around the 0.4-0.6 longitude range. This might suggest that channels from certain longitudes or regions tend to earn more, though further geographic analysis would provide clearer insights.

### Feature Selection using log Likelihood

Step 1: Compute log likelihood

```{r}
logLikelihood <- function(ytrue, ypred, epsilon=1e-6) {
sum(ifelse(ytrue==pos, log(ypred+epsilon), log(1-ypred-epsilon)), na.rm=T)
}
outcome <- 'earning_category'
logNull <- logLikelihood(
calibration_set[,outcome], sum(calibration_set[,outcome]==pos)/nrow(calibration_set)
)
cat(logNull)
```

Step 2: Run through categorical variables

```{r}
selCatVars <- c()
minDrop <- 5
for (v in catVars) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(calibration_set[,outcome], calibration_set[,pi]) - logNull)
  if (devDrop >= minDrop) {
    print(sprintf("%s, deviance reduction: %g", pi, devDrop))
    selCatVars <- c(selCatVars, pi)
  }
}
```

Based on the result here we observe that the predictor Country on the calibration set gives us the largest reduction to deviance.

Step 3: Run through numerical variables

```{r}
selNumVars <- c()
minDrop <- 5
for (v in numericVars) {
  pi <- paste('pred', v, sep='')
  devDrop <- 2*(logLikelihood(calibration_set[,outcome], calibration_set[,pi]) - logNull)
  if (devDrop >= minDrop) {
    print(sprintf("%s, deviance reduction: %g", pi, devDrop))
    selNumVars <- c(selNumVars, pi)
  }
}
```

Based on the result here we observe that the predictor Gross.tertiary.education.enrollment on the calibration set gives us the largest reduction to deviance.

According to the above results, we adopt these classification variables and numerical variables as the first group of feature variables.

```{r}
selVars = c(selCatVars, selNumVars)
selVars
```

### ROC Curves for selected categorical and numerical variables

```{r}
plot_roc <- function(predcol, outcol, colour_id=2, overlaid=F) {
ROCit_obj <- rocit(score=predcol, class=outcol==pos)
par(new=overlaid)
plot(ROCit_obj, col = c(colour_id, 1),
legend = FALSE, YIndex = FALSE, values = FALSE)
}
```

```{r message=FALSE, warning=FALSE}
plot_roc(calibration_set$predCountry, calibration_set[,"earning_category"], colour_id = 'purple', overlaid = T)
plot_roc(calibration_set$predvideo.views, calibration_set[,"earning_category"], colour_id = 'red', overlaid = T)
plot_roc(calibration_set$preduploads, calibration_set[,"earning_category"], colour_id = 'green', overlaid = T)
plot_roc(calibration_set$predGross.tertiary.education.enrollment...., calibration_set[,"earning_category"], colour_id = 'yellow', overlaid = T)
plot_roc(calibration_set$predPopulation, calibration_set[,"earning_category"], colour_id = 'black', overlaid = T)
plot_roc(calibration_set$predUnemployment.rate, calibration_set[,"earning_category"], colour_id = 'blue', overlaid = T)
plot_roc(calibration_set$predUrban_population, calibration_set[,"earning_category"], colour_id = 'pink', overlaid = T)
plot_roc(calibration_set$predLatitude, calibration_set[,"earning_category"], colour_id = 'cyan', overlaid = T)
plot_roc(calibration_set$predLongitude, calibration_set[,"earning_category"], colour_id = 'gold', overlaid = T)
```

Comparing each selected variable on the ROC Curve, we can see the red line(predvideo.views) and green line(preduploads) are worse variables compared to the other variables.

## Multivariate models

### Decision Tree

Building decision tree models using rpart()

```{r}
tVars <- paste('pred', c(catVars, numericVars), sep='')
(fV <- paste(outcome,'> 0 ~ ',
paste(tVars, collapse=' + '), sep=''))
tmodel <- rpart(fV, data = train_set)

print(calcAUC(predict(tmodel, newdata=train_set), train_set[,outcome]))
print(calcAUC(predict(tmodel, newdata=test_set), test_set[,outcome]))
print(calcAUC(predict(tmodel, newdata=calibration_set), calibration_set[,outcome]))
```

### Performance Measures

The following displays the performance measures in terms of accuracy, precision, recall and f1 score.

```{r}
logLikelihood <- function(ytrue, ypred, epsilon=1e-6) {
sum(ifelse(ytrue, log(ypred+epsilon), log(1-ypred+epsilon)), na.rm=T)
}
performanceMeasures <- function(ytrue, ypred, model.name = "model", threshold=0.5) {
# Compute the normalised deviance
dev.norm <- -2 * logLikelihood(ytrue, ypred)/length(ypred)
# Compute the confusion matrix
cmat <- table(actual = ytrue, predicted = ypred >= threshold)
accuracy <- sum(diag(cmat)) / sum(cmat)
precision <- cmat[2, 2] / sum(cmat[, 2])
recall <- cmat[2, 2] / sum(cmat[2, ])
f1 <- 2 * precision * recall / (precision + recall)
data.frame(model = model.name, precision = precision,
recall = recall, f1 = f1, dev.norm = dev.norm)
}

```

### Pander Formatting

```{r}
panderOpt <- function(){
library(pander)
# setting up Pander Options
panderOptions("plain.ascii", TRUE)
panderOptions("keep.trailing.zeros", TRUE)
panderOptions("table.style", "simple")
}
```

Prettier performance table function

```{r}
pp_table = function(model, xtrain, ytrain, xtest, ytest, xcal, ycal, threshold=0.5){
  panderOpt()
  perf_justify = 'lrrrrr'
  
  pred_train = predict(model, newdata = xtrain)
  pred_test = predict(model, newdata = xtest)
  pred_cal = predict(model, newdata = xcal)
  
  train_df = performanceMeasures(ytrain,pred_train, model.name = 'training', threshold=threshold)
  cal_df = performanceMeasures(ycal,pred_cal, model.name = 'calibration', threshold=threshold)
  test_df = performanceMeasures(ytest,pred_test, model.name = 'test', threshold=threshold)
  
  perftable <- rbind(train_df, cal_df, test_df)
  pandoc.table(perftable)
}
```

Print a performance table

```{r}
pp_table(tmodel, train_set[tVars],train_set[,outcome]==pos,test_set[tVars],test_set[,outcome]==pos, calibration_set[tVars],calibration_set[,outcome]==pos)
```

The model exhibits strong performance on the training set, with high precision, recall, and F1 score, signifying its effectiveness in identifying positive instances within the training data. However, it performs less optimally on the calibration set, displaying lower precision, recall and F1 score, which suggests potential overfitting or inadequate generalization to new data. On the test set, the model shows moderate performance with relatively high accuracy, precision, and recall.

### Plotting the AUC
```{r}
plot_roc <- function(predcoltrain, outcoltrain, predcolcal, outcolcal,predcoltest, outcoltest){
roc_train <- rocit(score=predcoltrain, class=outcoltrain==pos)
roc_cal <- rocit(score=predcolcal, class=outcolcal==pos)
roc_test <- rocit(score=predcoltest,class=outcoltest==pos)

plot(roc_train, col = c("blue","green"), lwd = 3, legend = FALSE,YIndex = FALSE, values = TRUE, asp=1)
lines(roc_cal$TPR ~ roc_cal$FPR, lwd = 3, col = c("red","green"), asp=1)
lines(roc_test$TPR ~ roc_test$FPR, lwd = 3, col = c("purple","green"), asp=1)
legend("bottomright", col = c("blue","red", "purple"),
c("Training", "Calibration", "Test"), lwd = 2)
}
pred_test_roc <- predict(tmodel, newdata=test_set)
pred_train_roc <- predict(tmodel, newdata=train_set)
pred_cal_roc <- predict(tmodel, newdata = calibration_set)

plot_roc(
  pred_train_roc, train_set[[outcome]], pred_cal_roc, calibration_set[[outcome]], pred_test_roc, test_set[[outcome]])
```

We can observe that training set and test set are quite good.

### Using Selected Features

```{r}
selVars

f <- paste(outcome,'>0 ~ ',
paste(selVars, collapse=' + '), sep='')
tmodel2 <- rpart(f, data=train_set)
print(calcAUC(predict(tmodel2, newdata=train_set[selVars]), train_set[,outcome]))
print(calcAUC(predict(tmodel2, newdata=test_set[selVars]), test_set[,outcome]))
print(calcAUC(predict(tmodel2, newdata=calibration_set[selVars]), calibration_set[,outcome]))
```
Print a performance table

```{r}
pp_table(tmodel2, train_set[tVars],train_set[,outcome]==pos,test_set[tVars],test_set[,outcome]==pos,calibration_set[tVars],calibration_set[,outcome]==pos)
```
The model indicate strong performance on the training set with high precision, recall, and F1 score, signifying effective identification of positives. However, performance on the calibration and test sets is relatively lower, with decreased precision, recall, and F1 score, suggesting potential overfitting issues and limited generalization to new data. The higher deviance normalization (dev.norm) values on the calibration and test sets indicate bias in model fit.

### Visualising a Decision Tree

```{r}
par(cex=0.5)
plot(tmodel2)
text(tmodel2)
```

### Plotting the ROC

```{r}
pred_test_roc2 <- predict(tmodel2, newdata=test_set)
pred_train_roc2 <- predict(tmodel2, newdata=train_set)
pred_cal_roc2 <- predict(tmodel2, newdata = calibration_set)

plot_roc(
  pred_train_roc2, train_set[[outcome]], pred_cal_roc2, calibration_set[[outcome]], pred_test_roc2, test_set[[outcome]])
```

We can find the training set performs best, calibration and test set are similar.

## Classification Models (Logistic Regression model)

```{r}
# Fitting logistic regression model
formula <- paste(outcome, '> 0 ~ ', paste(selVars, collapse=' + '), sep='')
```

```{r}
model_logr <- glm(formula=formula, data=train_set, family=binomial(link="logit"))
train_set$pred <- predict(model_logr, newdata=train_set, type="response")
test_set$pred <- predict(model_logr, newdata=test_set, type="response")
calibration_set$pred <- predict(model_logr, newdata=calibration_set, type="response")
```

Evaluate Performance

```{r}
train_performance <- performanceMeasures(train_set[, outcome], train_set$pred, model.name = 'training', threshold = 0.5)
test_performance <- performanceMeasures(test_set[, outcome], test_set$pred, model.name = 'test', threshold = 0.5)
cal_performance <- performanceMeasures(calibration_set[, outcome], calibration_set$pred, model.name = 'calibration', threshold = 0.5)
performance_table <- rbind(train_performance, cal_performance, test_performance)
pandoc.table(performance_table)
```
It performs well on the training set with a good balance of precision, recall, and F1 score, indicating effective identification of positive cases. However, on the calibration set, performance is slightly lower, with a modest drop in precision, recall, and F1 score, suggesting that the model may not generalize as well to new data. The test set performance is also reasonable, although it exhibits lower recall, which implies a potential challenge in correctly identifying positive cases. The deviance normalization (dev.norm) values are relatively low across all sets, indicating a good model fit with low bias. 

### Plotting the AUC (Logistic Regression model)

```{r}
plot_roc(
  train_set$pred, train_set[[outcome]], calibration_set$pred, calibration_set[[outcome]], test_set$pred, test_set[[outcome]])
```

We can find the training set performs best, calibration and test set are similar.

### LIME for the Logistic Regression Model

```{r}
explainer <- lime(train_set[selVars], model = model_logr, 
                  bin_continuous = TRUE, n_bins = 10)

model_type.glm <- function(x){
  return("regression") # for regression problem
}

predict_model.glm <- function(x, newdata, type = "response") {

    # return prediction value
    predict(x, newdata) %>% as.data.frame()
    
}
cases <- c(1,21,18,30)
(example <- test_set[cases,selVars])
explanation <- lime::explain(example, explainer, dist_fun = "manhattan",
                       kernel_width = 2, n_features = 4)

# Generate the plot
p <- plot_features(explanation = explanation)

# Adjust the plot theme
p <- p + theme(
  text = element_text(size = 9),
  axis.title.x = element_text(face = "bold", size = 10),
  axis.title.y = element_text(face = "bold", size = 10),
  axis.text.x = element_text(angle = 45, hjust = 0.5, size = 8),
  strip.text.x = element_text(hjust = 0, size = 7, face = "bold", vjust = 1.5),
  plot.margin = margin(t = 10, r = 5, b = 10, l = 5),
  legend.text = element_text(face = "bold", size = 10)
)

print(p)
```

We can see that predCountry and predvideo.views consistently emerge as influential factors across the cases, though their impact direction varies. In contrast, features like predGross.tertiary.education.enrollment and predUnemployment.rate have relatively minor influences. 

## Feature selection using Fisher Score

```{r}
# Calculate Fisher's Score
calculate_fisher_score <- function(feature, outcome) {
  positive_values <- feature[outcome == 1]
  negative_values <- feature[outcome == 0]
  
# Check if the set of positive and negative examples is empty
  if (length(positive_values) == 0 || length(negative_values) == 0) {
    return(0)  # Returns 0 or other appropriate value, indicating that the feature is not relevant
  }
  
  mean_positive <- mean(positive_values)
  mean_negative <- mean(negative_values)
  var_positive <- var(positive_values)
  var_negative <- var(negative_values)
  
  # Check if the variance is zero and avoid having a zero denominator
  if (var_positive == 0 || var_negative == 0) {
    return(0)  # Returns 0 or other appropriate value, indicating that the feature is not relevant
  }
  
  fisher_score <- ((mean_positive - mean_negative)^2) / (var_positive + var_negative)
  return(fisher_score)
}

# select feature variables
features <- youtube[, c("subscribers", "video.views", "uploads", "created_year", "created_month", "Gross.tertiary.education.enrollment....", "Population", "Unemployment.rate", "Urban_population", "Latitude", "Longitude","earning_category")]

# Perform one hot encoding on factor columns
categorical_features <- youtube[, c("category", "Country", "channel_type")]
encoded_features <- model.matrix(~ . - 1, data = categorical_features)

# Merge numerical columns and factor columns after unique heat coding
all_features <- cbind(features, encoded_features)

# Calculate Fisher's Score for each feature
fisher_scores <- sapply(all_features, calculate_fisher_score, outcome = outcome)

# Sort in descending order by Fisher's Score
sorted_features <- names(sort(fisher_scores, decreasing = TRUE))

# Select the top N features as the most relevant features
N <- 7  
selected_features <- sorted_features[1:N]

print(selected_features)
```

### ROC Curves for selected categorical and numerical variables

```{r message=FALSE, warning=FALSE}
plot_roc <- function(predcol, outcol, colour_id=2, overlaid=F) {
ROCit_obj <- rocit(score=predcol, class=outcol==pos)
par(new=overlaid)
plot(ROCit_obj, col = c(colour_id, 1),
legend = FALSE, YIndex = FALSE, values = FALSE)
}
plot_roc(calibration_set$predsubscribers, calibration_set[,"earning_category"], colour_id = 'orange', overlaid = T)
plot_roc(calibration_set$predvideo.views, calibration_set[,"earning_category"], colour_id = 'red', overlaid = T)
plot_roc(calibration_set$preduploads, calibration_set[,"earning_category"], colour_id = 'green', overlaid = T)
plot_roc(calibration_set$predcreated_year, calibration_set[,"earning_category"], colour_id = 'yellow', overlaid = T)
plot_roc(calibration_set$predcreated_month, calibration_set[,"earning_category"], colour_id = 'black', overlaid = T)
plot_roc(calibration_set$predGross.tertiary.education.enrollment...., calibration_set[,"earning_category"], colour_id = 'blue', overlaid = T)
plot_roc(calibration_set$predPopulation, calibration_set[,"earning_category"], colour_id = 'pink', overlaid = T)
```

We can find that only Population and Gross.tertiary.education.enrollment is better variable.

### Decision Tree

```{r}
fVars <- paste('pred', selected_features, sep='')
(fV <- paste(outcome,'> 0 ~ ',
paste(fVars, collapse=' + '), sep=''))
tmodel3 <- rpart(fV, data = train_set)

# To inspect the model, type: summary (tmodel)
print(calcAUC(predict(tmodel3, newdata=train_set), train_set[,outcome]))
print(calcAUC(predict(tmodel3, newdata=test_set), test_set[,outcome]))
print(calcAUC(predict(tmodel3, newdata=calibration_set), calibration_set[,outcome]))
```
Print a performance table

```{r}
pp_table(tmodel3, train_set[fVars],train_set[,outcome]==pos,test_set[fVars],test_set[,outcome]==pos,calibration_set[tVars],calibration_set[,outcome]==pos)
```
The model indicates strong performance on the training set with high precision but lower recall, suggesting the possibility of missing some positive instances, resulting in a moderate F1 score. On the calibration set, performance is suboptimal, with reduced precision and recall, indicating the need for improvement in capturing positive cases. The test set performance is relatively good, characterized by high precision but slightly lower recall, resulting in a moderate F1 score. The deviance normalization (dev.norm) values suggest a reasonably good model fit, although there is a slight bias on the calibration set.

### Visualising a Decision Tree

```{r}
par(cex=0.5)
plot(tmodel)
text(tmodel)
```

### Plotting the AUC

```{r}
plot_roc <- function(predcoltrain, outcoltrain, predcolcal, outcolcal,predcoltest, outcoltest){
roc_train <- rocit(score=predcoltrain, class=outcoltrain==pos)
roc_cal <- rocit(score=predcolcal, class=outcolcal==pos)
roc_test <- rocit(score=predcoltest,class=outcoltest==pos)

plot(roc_train, col = c("blue","green"), lwd = 3, legend = FALSE,YIndex = FALSE, values = TRUE, asp=1)
lines(roc_cal$TPR ~ roc_cal$FPR, lwd = 3, col = c("red","green"), asp=1)
lines(roc_test$TPR ~ roc_test$FPR, lwd = 3, col = c("purple","green"), asp=1)
legend("bottomright", col = c("blue","red", "purple"),
c("Training", "Calibration", "Test"), lwd = 2)
}
pred_test_roc3 <- predict(tmodel3, newdata=test_set)
pred_train_roc3 <- predict(tmodel3, newdata=train_set)
pred_cal_roc3 <- predict(tmodel3, newdata = calibration_set)

plot_roc(
  pred_train_roc3, train_set[[outcome]], pred_cal_roc3, calibration_set[[outcome]], pred_test_roc3, test_set[[outcome]])
```

We can see trainging and calibration set perform better than test set.

### Logistic Regression Model

```{r}
formula <- paste(outcome, '> 0 ~ ', paste(fVars, collapse=' + '), sep='')

model_logr <- glm(formula=formula, data=train_set, family=binomial(link="logit"))
train_set$pred2 <- predict(model_logr, newdata=train_set, type="response")
test_set$pred2 <- predict(model_logr, newdata=test_set, type="response")
calibration_set$pred2 <- predict(model_logr, newdata=calibration_set, type="response")

train_performance2 <- performanceMeasures(train_set[, outcome], train_set$pred2, model.name = 'training', threshold = 0.5)
test_performance2 <- performanceMeasures(test_set[, outcome], test_set$pred2, model.name = 'test', threshold = 0.5)
cal_performance2 <- performanceMeasures(calibration_set[, outcome], calibration_set$pred2, model.name = 'calibration', threshold = 0.5)

performance_table2 <- rbind(train_performance2, cal_performance2, test_performance2)

pandoc.table(performance_table2)
```
The model exhibits good performance, particularly on the training and test sets. However, there is room for improvement in recall, especially on the test set. The model demonstrates a reasonably good fit with low bias.

### Plotting the AUC

```{r}
plot_roc(
  train_set$pred2, train_set[[outcome]], calibration_set$pred2, calibration_set[[outcome]], test_set$pred2, test_set[[outcome]])
```

We can see there is no obvious difference between each set.

### LIME for the Logistic Regression Model

```{r}
explainer <- lime(train_set[fVars], model = model_logr, 
                  bin_continuous = TRUE, n_bins = 10)

model_type.glm <- function(x){
  return("regression") # for regression problem
}

predict_model.glm <- function(x, newdata, type = "response") {

    # return prediction value
    predict(x, newdata) %>% as.data.frame()
    
}
cases <- c(1,3,6,11)
(example <- test_set[cases,fVars])
explanation <- lime::explain(example, explainer,dist_fun = "manhattan",
                       kernel_width = 2, n_features = 4)

# Generate the plot
p <- plot_features(explanation = explanation)

# Adjust the plot theme
p <- p + theme(
  text = element_text(size = 9),
  axis.title.x = element_text(face = "bold", size = 10),
  axis.title.y = element_text(face = "bold", size = 10),
  axis.text.x = element_text(angle = 45, hjust = 0.5, size = 8),
  strip.text.x = element_text(hjust = 0, size = 6, face = "bold", vjust = 1.5),
  plot.margin = margin(t = 10, r = 5, b = 10, l = 5),
  legend.text = element_text(face = "bold", size = 10)
)
# Print the modified plot
print(p)
```

We can see that across all cases, predvideo.views consistently has a major influence on the predictions. Other features vary in their impact, but predPopulation and predGross.tertiary.education.enrollment often emerge as significant. The time of content creation, whether by month or year, also seems to play a role, but its influence varies.

## Model Comparision

In this section, we compare different models for different selected features by comparing the accuracy, precision, recall, f1 and dev,norm.

```{r}
pp_table(tmodel2, train_set[tVars],train_set[,outcome]==pos,test_set[tVars],test_set[,outcome]==pos,calibration_set[tVars],calibration_set[,outcome]==pos)
```

```{r}
pandoc.table(performance_table)
```

```{r}
pp_table(tmodel3, train_set[tVars],train_set[,outcome]==pos,test_set[tVars],test_set[,outcome]==pos,calibration_set[tVars],calibration_set[,outcome]==pos)
```

```{r}
pandoc.table(performance_table2)
```
#### First Model (Log Likelihood Feature Selection + Decision Tree Model)
Performs well on the training set with high precision and F1 score, but has a slightly lower recall, potentially missing some positive instances.
Suboptimal performance on the calibration and test sets with lower precision and F1 score, especially on the calibration set, suggesting potential overfitting.

#### Second Model (Log Likelihood Feature Selection + Logistic Regression Model)
Demonstrates good precision and F1 score on the training set, but recall is relatively lower, indicating room for improvement in capturing positive instances.
Relatively balanced performance on the calibration and test sets with a moderate F1 score, suggesting reasonable generalization.

#### Third Model (Fisher Score Feature Selection + Decision Tree Model)
Shows strong performance on the training set with high precision, recall, and F1 score, indicating excellent training fit.
Maintains balanced performance on the calibration and test sets with high precision, recall, and F1 score, showcasing good generalization.

#### Fourth Model (Fisher Score Feature Selection + Logistic Regression Model)
Performs well on the training set with high precision, recall, and F1 score, suggesting a strong training fit.
Achieves relatively balanced performance on the calibration and test sets with a moderate F1 score, implying reasonable generalization.

In summary, the third model (Fisher Score Feature Selection + Decision Tree Model) excels in terms of precision, recall, and F1 score on all datasets, indicating robust performance and excellent generalization. The second model (Log Likelihood Feature Selection + Logistic Regression Model) also exhibits favorable performance, particularly on the calibration and test sets. The first model (Log Likelihood Feature Selection + Decision Tree Model) excels on the training set but may face overfitting issues. The fourth model (Fisher Score Feature Selection + Logistic Regression Model) performs well on the training set but slightly underperforms on the test set. 

```{r}
par(mfrow = c(2, 2))

plot_roc(pred_train_roc2, train_set[[outcome]], pred_cal_roc2, calibration_set[[outcome]], pred_test_roc2, test_set[[outcome]])
plot_roc(
  train_set$pred, train_set[[outcome]], calibration_set$pred, calibration_set[[outcome]], test_set$pred, test_set[[outcome]])
plot_roc(
  pred_train_roc3, train_set[[outcome]], pred_cal_roc3, calibration_set[[outcome]], pred_test_roc3, test_set[[outcome]])
plot_roc(
  train_set$pred2, train_set[[outcome]], calibration_set$pred2, calibration_set[[outcome]], test_set$pred2, test_set[[outcome]])
```

Feature selection methods (Log Likelihood vs. Fisher Score) and model types (Decision Tree vs. Logistic Regression) lead to different performance profiles. Logistic Regression, regardless of the feature selection method, tends to generalize better to unseen data, as seen in the Test curves. Among all configurations, Logistic Regression with features selected by Fisher Score appears to offer the most balanced and reliable performance across Training, Calibration, and Test datasets.

# Part 3: Clustering

Make clustering more coordinate-free

```{r message=FALSE, warning=FALSE}
youtube <- youtube[youtube$Country != "missing", ] # Delete the missing value
# Classify the country, keeping only one column for countries that appear multiple times, and replace the remaining variables with averages
youtube <- youtube %>%
  group_by(Country) %>%
  summarise(across(everything(), mean, na.rm = TRUE))
selected_features_df <- youtube[, selected_features]
scaled_df <- scale(selected_features_df)
```

### Finding K

```{r}
# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
sum((x - y)^2)
}
# Function to calculate WSS of a cluster, represented as a n-by-d matrix
# (where n and d are the numbers of rows and columns of the matrix)
# which contains only points of the cluster.
wss <- function(clustermat) {
c0 <- colMeans(clustermat)
sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
# Function to calculate the total WSS. Argument `scaled_df`: data frame
# with normalised numerical columns. Argument `labels`: vector containing
# the cluster ID (starting at 1) for each row of the data frame.
wss_total <- function(scaled_df, labels) {
wss.sum <- 0
k <- length(unique(labels))
for (i in 1:k)
wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
wss.sum
}

# Function to calculate total sum of squared (TSS) distance of data
# points about the (global) mean. This is the same as WSS when the
# number of clusters (k) is 1.
tss <- function(scaled_df) {
wss(scaled_df)
}

# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
# for a vector of k values ranging from 1 to kmax.
CH_index <- function(scaled_df, kmax, method="kmeans") {
if (!(method %in% c("kmeans", "hclust")))
stop("method must be one of c('kmeans', 'hclust')")
npts <- nrow(scaled_df)
wss.value <- numeric(kmax) # create a vector of numeric type
# wss.value[1] stores the WSS value for k=1 (when all the
# data points form 1 large cluster).
wss.value[1] <- wss(scaled_df)
if (method == "kmeans") {
# kmeans
for (k in 2:kmax) {
clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
wss.value[k] <- clustering$tot.withinss
}
} else {
# hclust
d <- dist(scaled_df, method="euclidean")
pfit <- hclust(d, method="ward.D2")
for (k in 2:kmax) {
labels <- cutree(pfit, k=k)
wss.value[k] <- wss_total(scaled_df, labels)
}
}
bss.value <- tss(scaled_df) - wss.value # This is a vector
B <- bss.value / (0:(kmax-1)) # also a vector
W <- wss.value / (npts - 1:kmax) # also a vector
data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

### Plot the CH and WSS Index

```{r message=FALSE, warning=FALSE}
# Calculate the CH criterion
crit.df <- CH_index(scaled_df, 10, method="hclust")
fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)
```

Looking at the figure, we see that the CH criterion is maximised at k = 6. There is no clear “elbow” in the WSS curve. Our choice of 6 clusters seems reasonable.

## Hierarchical Clustering

```{r}
d <- dist(scaled_df, method="euclidean")
pfit <- hclust(d, method="ward.D2") # perform hierarchical clustering
plot(pfit,labels=youtube$Country, main="Cluster Dendrogram for Country")
rect.hclust(pfit, k=6) # k = 6 means we want rectangles to be put around 6 clusters
```

We employed the Euclidean distance metric to build the dendrogram.

The choice of the Euclidean distance measure for clustering was made due to its geometric intuitiveness and widespread applicability. It calculates the straight-line distance between data points, considering the magnitude of differences, making it suitable for continuous data. However, its impact on clustering outcomes can vary; it assumes spherical and equally sized clusters, making it less effective for non-spherical or high-dimensional data, sensitive to outliers, and affected by feature scaling.

Leveraging the insights gained from our prior implementation of the k-means algorithm, we can observe how the dendrogram partitions the hierarchical structure into six distinct clusters as we make the cuts from the bottom-up approach.

### Visualising Clusters - Data Preparation

```{r}
groups <- cutree(pfit, k=6)

print_clusters <- function(df, groups, cols_to_print) {
Ngroups <- max(groups)
for (i in 1:Ngroups) {
print(paste("cluster", i))
print(df[groups == i, cols_to_print])
}
}

princ <- prcomp(scaled_df)
nComp <- 2 
project2D <- as.data.frame(predict(princ, newdata=scaled_df)[,1:nComp])
hclust.project2D <- cbind(project2D, cluster=as.factor(groups), country=youtube$Country)
head(hclust.project2D)
```

### Visualising Clusters - Finding the Convex Hull

```{r}
find_convex_hull <- function(proj2Ddf, groups) {
do.call(rbind,
lapply(unique(groups),
FUN = function(c) {
f <- subset(proj2Ddf, cluster==c);
f[chull(f),]
}
)
)
}
hclust.hull <- find_convex_hull(hclust.project2D, groups)
```

### Visualising Clusters

```{r}
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
  geom_point(aes(shape=as.factor(cluster), color=cluster)) +
  scale_shape_manual(values = c(1, 2, 3, 4, 5, 6, 7)) +
  geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
               alpha=0.4, linetype=0) +
  theme(text=element_text(size=10))
```

This is a two-dimensional scatter plot showing data distribution across PC1 and PC2 principal components, differentiated into six distinct clusters using varied markers and colors, with data points in each cluster grouped closely together, forming six distinct regions.

The plot displays some overlap between clusters, particularly between the green, yellow, and teal regions, indicating potential similarities or ambiguities in those data groupings within the PC1 and PC2 dimensions.

### Using Clusterboot

We use clusterboot to find out how stable the clustering algorithm is.

```{r}
kbest.p <- 6
cboot.hclust <- clusterboot(scaled_df, clustermethod=hclustCBI,
method="ward.D2", k=kbest.p)
summary(cboot.hclust$result)
```

Print the result of each cluster

```{r}
groups.cboot <- cboot.hclust$result$partition
print_clusters(youtube, groups.cboot, "Country")
```

### Finding the Stable Clusters

```{r}
(values <- 1 - cboot.hclust$bootbrd/100) # large values here => highly stable
cat("So clusters", order(values)[5], "and", order(values)[4], "are highly stable")
```

# Conclusion
We undertook a detailed analysis of the YouTube dataset, emphasizing the classification of YouTube channels based on their earnings. We categorized channels into "high-earning (1)" and "low-earning (0)" groups, with the GDP of the channel's home country as a key factor. Through univariate analysis, we identified key variables using log likelihood and the Fisher score, which resulted in two comprehensive sets of feature variables. Leveraging these features, we employed multivariate models. The decision tree model provided a clear depiction of the hierarchical relationships in the data, while the logistic regression analysis offered insights into the likelihood of specific outcomes based on the features. Furthermore, clustering techniques were implemented to group data based on similarities, unveiling underlying patterns and structures. 


